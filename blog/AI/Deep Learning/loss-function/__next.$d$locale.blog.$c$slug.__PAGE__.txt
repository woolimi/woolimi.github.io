1:"$Sreact.fragment"
2:I[26334,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"BlurFade"]
3:I[935,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"default"]
8:I[49243,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"JupyterNotebookViewer",1]
9:I[64725,[],"OutletBoundary"]
a:"$Sreact.suspense"
:HL["/_next/static/css/911e6a603adbdfb3.css","style"]
0:{"buildId":"7wrLch1PHmN9S3I0q82uc","rsc":["$","$1","c",{"children":[["$","section",null,{"id":"blog","className":"mb-30","children":["$","$L2",null,{"delay":0.04,"duration":0.5,"blur":"4px","children":[["$","nav",null,{"aria-label":"breadcrumb","data-slot":"breadcrumb","className":"mb-6","children":["$","ol",null,{"data-slot":"breadcrumb-list","className":"text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5","children":[["$","li","item-0",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Home","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-1",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-1",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Blog","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-2",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-2",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog/AI","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"AI","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-3",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-3",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog/AI/Deep Learning","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Deep Learning","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-4",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-4",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","span",null,{"data-slot":"breadcrumb-page","role":"link","aria-disabled":"true","aria-current":"page","className":"text-foreground font-normal","children":"Loss Function"}]}]]}]}],["$","script",null,{"type":"application/ld+json","suppressHydrationWarning":true,"dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Loss Function\",\"description\":\"신경망에서 사용하는 손실 함수의 종류(MSE, MAE, Binary Crossentropy, Categorical Crossentropy 등)와 각각의 특징, 수식, 그리고 문제 유형에 따른 적절한 선택 방법을 학습한다\",\"image\":\"https://woolimi.github.io/og?title=Loss Function\",\"url\":\"https://woolimi.github.io/blog/AI/Deep Learning/loss-function\",\"author\":{\"@type\":\"Person\",\"name\":\"박우림\"}}"}}],"$L4","$L5"]}]}],["$L6"],"$L7"]}],"loading":null,"isPartial":false}
4:["$","h1",null,{"className":"text-3xl mb-4 md:mb-10 leading-none font-semibold tracking-tight sm:text-4xl sm:leading-none md:text-5xl md:leading-none lg:text-6xl lg:leading-none","children":"Loss Function"}]
5:["$","article",null,{"children":["$","$L8",null,{"notebook":{"cells":[{"cell_type":"markdown","source":["\n","## 손실 함수(Loss Function)란?\n","\n","손실 함수는 모델의 예측값과 실제값 사이의 차이를 측정하는 함수이다. 모델은 손실 함수의 값을 최소화하는 방향으로 학습하며, 문제의 유형(회귀, 분류)에 따라 적절한 손실 함수를 선택해야 한다.\n","\n","## 회귀 문제의 손실 함수\n","\n","### 1. MSE (Mean Squared Error)\n","\n","**MSE**는 예측값과 실제값의 차이를 제곱하여 평균을 낸 값이다.\n","\n","$$$L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n","\n","**특징:**\n","- 큰 오차에 더 큰 페널티를 부여한다\n","- 이상치(outlier)에 민감하다\n","- 미분 가능하여 경사 하강법에 적합하다\n","\n","**용도:**\n","- 일반적인 회귀 문제\n","- 이상치가 적고 정규 분포에 가까운 데이터\n","- 큰 오차를 강하게 처벌하고 싶을 때\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","model.compile(optimizer='adam', loss='mse')\n","```\n","\n","### 2. MAE (Mean Absolute Error)\n","\n","**MAE**는 예측값과 실제값의 차이의 절댓값을 평균낸 값이다.\n","\n","$$$L = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n","\n","**특징:**\n","- 모든 오차에 동일한 가중치를 부여한다\n","- 이상치에 덜 민감하다 (robust)\n","- 미분 가능하지만 0에서 미분 불가능하다\n","\n","**용도:**\n","- 이상치가 많은 데이터\n","- 모든 오차를 동등하게 다루고 싶을 때\n","- 이상치의 영향을 줄이고 싶을 때\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","model.compile(optimizer='adam', loss='mae')\n","```\n","\n","### 3. Huber Loss\n","\n","**Huber Loss**는 MSE와 MAE를 결합한 손실 함수이다.\n","\n","수식:\n","\n","- $|y - \\hat{y}| \\leq \\delta$ 일 때: $L_\\delta = \\frac{1}{2}(y - \\hat{y})^2$\n","- $|y - \\hat{y}| > \\delta$ 일 때: $L_\\delta = \\delta|y - \\hat{y}| - \\frac{1}{2}\\delta^2$\n","\n","**특징:**\n","- 작은 오차는 MSE처럼 제곱, 큰 오차는 MAE처럼 선형으로 처리한다\n","- 이상치에 강건하면서도 미분 가능하다\n","- $\\delta$ 하이퍼파라미터를 조정할 수 있다\n","\n","**용도:**\n","- 이상치가 있지만 MSE의 장점도 활용하고 싶을 때\n","- MSE와 MAE의 중간 성격이 필요할 때\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","import tensorflow as tf\n","model.compile(optimizer='adam', loss=tf.keras.losses.Huber(delta=1.0))\n","```\n","\n","## 분류 문제의 손실 함수\n","\n","### 1. Binary Crossentropy\n","\n","**Binary Crossentropy**는 이진 분류 문제에서 사용하는 손실 함수이다.\n","\n","$$$L = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n","\n","**특징:**\n","- 출력층에 Sigmoid 활성화 함수와 함께 사용한다\n","- 예측 확률과 실제 레이블의 차이를 측정한다\n","- 확률 분포의 차이를 측정하는 크로스 엔트로피를 사용한다\n","\n","**용도:**\n","- **이진 분류**: 스팸 메일 분류, 질병 진단, 고객 이탈 예측 등\n","- 출력층: Sigmoid 활성화 함수\n","- 출력 뉴런 수: 1개\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(optimizer='adam', loss='binary_crossentropy')\n","```\n","\n","### 2. Categorical Crossentropy\n","\n","**Categorical Crossentropy**는 다중 분류 문제에서 사용하며, 레이블이 원-핫 인코딩된 경우에 사용한다.\n","\n","$$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{c=1}^{C}y_{i,c}\\log(\\hat{y}_{i,c})$$\n","\n","**특징:**\n","- 출력층에 Softmax 활성화 함수와 함께 사용한다\n","- 레이블이 원-핫 벡터 형태여야 한다 (예: [0, 0, 1, 0])\n","- 각 클래스에 대한 확률 분포를 비교한다\n","\n","**용도:**\n","- **다중 분류**: 이미지 분류, 감정 분석 등\n","- 출력층: Softmax 활성화 함수\n","- 레이블 형태: 원-핫 인코딩 (예: [0, 0, 1, 0])\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","# 레이블: [0, 0, 1, 0] (원-핫 인코딩)\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","```\n","\n","### 3. Sparse Categorical Crossentropy\n","\n","**Sparse Categorical Crossentropy**는 다중 분류 문제에서 사용하며, 레이블이 정수 형태인 경우에 사용한다.\n","\n","$$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\hat{y}_{i,y_i})$$\n","\n","**특징:**\n","- 출력층에 Softmax 활성화 함수와 함께 사용한다\n","- 레이블이 정수 형태여야 한다 (예: 2)\n","- Categorical Crossentropy와 수학적으로 동일하지만 레이블 형태가 다르다\n","\n","**용도:**\n","- **다중 분류**: 이미지 분류, 감정 분석 등\n","- 출력층: Softmax 활성화 함수\n","- 레이블 형태: 정수 (예: 2)\n","\n","**예시:**\n","```python\n","# TensorFlow/Keras\n","# 레이블: 2 (정수)\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","```\n","\n","## 손실 함수 선택 가이드\n","\n","| 문제 유형 | 손실 함수 | 출력층 활성화 함수 | 레이블 형태 | 예시 |\n","|----------|----------|------------------|------------|------|\n","| **회귀** | MSE | 없음 (Linear) | 실수값 | 주택 가격 예측 |\n","| **회귀 (이상치 많음)** | MAE | 없음 (Linear) | 실수값 | 이상치가 많은 데이터 |\n","| **이진 분류** | Binary Crossentropy | Sigmoid | 0 또는 1 | 스팸 메일 분류 |\n","| **다중 분류** | Categorical Crossentropy | Softmax | 원-핫 벡터 | 이미지 분류 (원-핫) |\n","| **다중 분류** | Sparse Categorical Crossentropy | Softmax | 정수 | 이미지 분류 (정수) |\n","\n","## 선택 시 고려사항\n","\n","### 회귀 문제\n","\n","1. **MSE vs MAE**\n","   - 이상치가 적고 큰 오차를 강하게 처벌: **MSE**\n","   - 이상치가 많고 모든 오차를 동등하게: **MAE**\n","   - 둘의 중간: **Huber Loss**\n","\n","2. **데이터 분포**\n","   - 정규 분포에 가까움: MSE\n","   - 이상치가 많음: MAE 또는 Huber Loss\n","\n","### 분류 문제\n","\n","1. **이진 vs 다중 분류**\n","   - 2개 클래스: **Binary Crossentropy**\n","   - 3개 이상 클래스: **Categorical Crossentropy** 또는 **Sparse Categorical Crossentropy**\n","\n","2. **레이블 형태**\n","   - 원-핫 인코딩: **Categorical Crossentropy**\n","   - 정수 레이블: **Sparse Categorical Crossentropy** (메모리 효율적)\n","\n","## 요약\n","\n","- **회귀**: MSE (일반), MAE (이상치 많음), Huber Loss (중간)\n","- **이진 분류**: Binary Crossentropy + Sigmoid\n","- **다중 분류**: Categorical Crossentropy (원-핫) 또는 Sparse Categorical Crossentropy (정수) + Softmax\n","\n","손실 함수는 모델의 학습 방향을 결정하므로, 문제의 특성과 데이터의 분포를 고려하여 적절한 함수를 선택하는 것이 중요하다.\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"language_info":{"name":"python"},"title":"Loss Function","summary":"신경망에서 사용하는 손실 함수의 종류(MSE, MAE, Binary Crossentropy, Categorical Crossentropy 등)와 각각의 특징, 수식, 그리고 문제 유형에 따른 적절한 선택 방법을 학습한다"}}}]}]
6:["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/911e6a603adbdfb3.css","precedence":"next"}]
7:["$","$L9",null,{"children":["$","$a",null,{"name":"Next.MetadataOutlet","children":"$@b"}]}]
b:null
