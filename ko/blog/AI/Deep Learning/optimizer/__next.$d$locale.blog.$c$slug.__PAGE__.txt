1:"$Sreact.fragment"
2:I[26334,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"BlurFade"]
3:I[935,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"default"]
8:I[49243,["1921","static/chunks/79812939-dd468ad34b0555ab.js","2969","static/chunks/8fdae66b-3313d3073a4d6626.js","9991","static/chunks/9991-ccaf069d2d1a3394.js","650","static/chunks/650-f48a6caa28b58c59.js","7285","static/chunks/7285-e0b078d0c178a272.js","2061","static/chunks/2061-28fac52e9cc6c1cf.js","1283","static/chunks/1283-41499aa8a4a2d40d.js","5441","static/chunks/5441-6139f8dec0b22e57.js","5784","static/chunks/app/%5Blocale%5D/blog/%5B...slug%5D/page-b33400911b20deaf.js"],"JupyterNotebookViewer",1]
9:I[64725,[],"OutletBoundary"]
a:"$Sreact.suspense"
:HL["/_next/static/css/911e6a603adbdfb3.css","style"]
0:{"buildId":"7wrLch1PHmN9S3I0q82uc","rsc":["$","$1","c",{"children":[["$","section",null,{"id":"blog","className":"mb-30","children":["$","$L2",null,{"delay":0.04,"duration":0.5,"blur":"4px","children":[["$","nav",null,{"aria-label":"breadcrumb","data-slot":"breadcrumb","className":"mb-6","children":["$","ol",null,{"data-slot":"breadcrumb-list","className":"text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5","children":[["$","li","item-0",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Home","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-1",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-1",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Blog","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-2",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-2",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog/AI","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"AI","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-3",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-3",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","$L3",null,{"ref":null,"href":"/ko/blog/AI/Deep Learning","localeCookie":{"name":"NEXT_LOCALE","sameSite":"lax"},"children":"Deep Learning","data-slot":"breadcrumb-link","className":"hover:text-foreground transition-colors"}]}],["$","li","separator-4",{"data-slot":"breadcrumb-separator","role":"presentation","aria-hidden":"true","className":"[&>svg]:size-3.5","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]}],["$","li","item-4",{"data-slot":"breadcrumb-item","className":"inline-flex items-center gap-1.5","children":["$","span",null,{"data-slot":"breadcrumb-page","role":"link","aria-disabled":"true","aria-current":"page","className":"text-foreground font-normal","children":"Optimizer"}]}]]}]}],["$","script",null,{"type":"application/ld+json","suppressHydrationWarning":true,"dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Optimizer\",\"description\":\"딥러닝 모델의 가중치를 업데이트하는 최적화 알고리즘들(Gradient Descent, SGD, Momentum, Adagrad, RMSProp, Adam)의 원리와 특징을 설명하고 비교한다\",\"image\":\"https://woolimi.github.io/og?title=Optimizer\",\"url\":\"https://woolimi.github.io/blog/AI/Deep Learning/optimizer\",\"author\":{\"@type\":\"Person\",\"name\":\"박우림\"}}"}}],"$L4","$L5"]}]}],["$L6"],"$L7"]}],"loading":null,"isPartial":false}
4:["$","h1",null,{"className":"text-3xl mb-4 md:mb-10 leading-none font-semibold tracking-tight sm:text-4xl sm:leading-none md:text-5xl md:leading-none lg:text-6xl lg:leading-none","children":"Optimizer"}]
5:["$","article",null,{"children":["$","$L8",null,{"notebook":{"cells":[{"cell_type":"markdown","source":["\n","## Optimizer 란?\n","\n","**Optimizer(최적화 알고리즘)**는 신경망 모델의 가중치(weights)와 편향(bias)을 업데이트하는 방법을 결정하는 알고리즘이다. 손실 함수(loss function)의 기울기(gradient)를 계산하고, 이를 바탕으로 모델 파라미터를 조정하여 손실을 최소화한다.\n","\n","### Gradient Descent\n","\n","**Gradient Descent(경사 하강법)**는 가장 기본적인 최적화 알고리즘이다.\n","\n","- **원리**: 손실 함수의 기울기(경사)를 따라 내려가면서 최소값을 찾는다\n","- **수식**: $w_{t+1} = w_t - \\eta \\nabla_w L(w_t)$\n","  - $w_t$: 현재 가중치\n","  - $\\eta$: 학습률(learning rate)\n","  - $\\nabla_w L(w_t)$: 손실 함수의 기울기\n","- **특징**:\n","  - 전체 데이터셋을 사용하여 기울기를 계산한다 (Batch Gradient Descent)\n","  - 안정적이지만 계산 비용이 크고, 지역 최소값에 빠질 수 있다\n","  - 학습률이 고정되어 있어 수렴 속도가 느릴 수 있다\n","\n","### SGD: Stochastic Gradient Descent\n","\n","**SGD(확률적 경사 하강법)**는 Gradient Descent의 변형이다.\n","\n","- **원리**: 전체 데이터셋 대신 **하나의 샘플**을 무작위로 선택하여 기울기를 계산한다\n","- **수식**: $w_{t+1} = w_t - \\eta \\nabla_w L(w_t, x_i, y_i)$\n","  - $(x_i, y_i)$: 무작위로 선택된 하나의 샘플\n","- **특징**:\n","  - 계산이 빠르고 메모리 사용량이 적다\n","  - 노이즈가 있어 지역 최소값에서 탈출할 수 있다\n","  - 수렴이 불안정하고 진동이 발생할 수 있다\n","  - Mini-batch SGD: 여러 샘플을 묶어서 사용하는 중간 형태가 일반적이다\n","\n","### Momentum\n","\n","**Momentum(모멘텀)**은 물리학의 관성 개념을 적용한 최적화 알고리즘이다.\n","\n","- **원리**: 과거의 기울기 방향을 기억하여 일관된 방향으로 이동한다\n","- **수식**: \n","  - $v_t = \\beta v_{t-1} + \\eta \\nabla_w L(w_t)$\n","  - $w_{t+1} = w_t - v_t$\n","  - $\\beta$: 모멘텀 계수(보통 0.9)\n","- **특징**:\n","  - 진동을 줄이고 수렴 속도를 향상시킨다\n","  - 지역 최소값을 넘어서 더 나은 최소값을 찾을 수 있다\n","  - SGD보다 안정적이고 빠른 수렴을 보인다\n","\n","### Adagrad\n","\n","**Adagrad(Adaptive Gradient)**는 각 파라미터마다 학습률을 자동으로 조정한다.\n","\n","- **원리**: 자주 업데이트되는 파라미터는 작은 학습률을, 드물게 업데이트되는 파라미터는 큰 학습률을 사용한다\n","- **수식**:\n","  - $G_t = G_{t-1} + (\\nabla_w L(w_t))^2$\n","  - $w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla_w L(w_t)$\n","  - $G_t$: 지금까지의 기울기 제곱의 누적합\n","  - $\\epsilon$: 0으로 나누는 것을 방지하는 작은 값\n","- **특징**:\n","  - 희소한(sparse) 데이터에 효과적이다\n","  - 학습률이 시간이 지날수록 작아져서 학습이 멈출 수 있다\n","  - 학습률을 수동으로 조정할 필요가 없다\n","\n","### RMSProp\n","\n","**RMSProp(Root Mean Square Propagation)**는 Adagrad의 단점을 개선한 알고리즘이다.\n","\n","- **원리**: 과거 기울기의 제곱을 지수 이동 평균으로 계산하여 최근 기울기에 더 큰 가중치를 부여한다\n","- **수식**:\n","  - $E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta)(\\nabla_w L(w_t))^2$\n","  - $w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_w L(w_t)$\n","  - $\\beta$: 감쇠율(보통 0.9)\n","- **특징**:\n","  - Adagrad의 학습률 감소 문제를 해결한다\n","  - 비정상(non-stationary) 문제에 효과적이다\n","  - 순환 신경망(RNN)에서 좋은 성능을 보인다\n","\n","### Adam\n","\n","**Adam(Adaptive Moment Estimation)**는 Momentum과 RMSProp를 결합한 알고리즘이다.\n","\n","- **원리**: 기울기의 1차 모멘트(평균)와 2차 모멘트(분산)를 모두 추정하여 적응적 학습률을 사용한다\n","- **수식**:\n","  - $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_w L(w_t)$ (1차 모멘트)\n","  - $v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla_w L(w_t))^2$ (2차 모멘트)\n","  - $\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$, $\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$ (편향 보정)\n","  - $w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$\n","  - $\\beta_1$: 보통 0.9, $\\beta_2$: 보통 0.999\n","- **특징**:\n","  - 가장 널리 사용되는 최적화 알고리즘이다\n","  - 빠른 수렴 속도와 안정적인 성능을 보인다\n","  - 하이퍼파라미터 튜닝이 비교적 쉽다\n","  - 대부분의 딥러닝 문제에서 좋은 성능을 보인다\n","\n","## Optimizer 비교\n","\n","| Optimizer | 수렴 속도 | 안정성 | 메모리 | 특징 |\n","|-----------|----------|--------|--------|------|\n","| **Gradient Descent** | 느림 | 높음 | 높음 | 전체 데이터셋 사용, 안정적 |\n","| **SGD** | 보통 | 낮음 | 낮음 | 빠르지만 불안정, 진동 발생 |\n","| **Momentum** | 빠름 | 보통 | 낮음 | 진동 감소, 일관된 방향 이동 |\n","| **Adagrad** | 보통 | 보통 | 보통 | 희소 데이터에 효과적, 학습률 감소 문제 |\n","| **RMSProp** | 빠름 | 높음 | 보통 | Adagrad 개선, 비정상 문제에 효과적 |\n","| **Adam** | 매우 빠름 | 높음 | 보통 | 가장 널리 사용, Momentum + RMSProp 결합 |\n","\n","### 선택 가이드\n","\n","- **작은 데이터셋, 간단한 모델**: SGD 또는 Momentum\n","- **희소한 데이터**: Adagrad\n","- **순환 신경망(RNN)**: RMSProp\n","- **대부분의 경우**: **Adam** (권장)\n","- **안정적인 학습이 중요한 경우**: Gradient Descent 또는 Momentum\n","\n"],"outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"language_info":{"name":"python"},"title":"Optimizer","summary":"딥러닝 모델의 가중치를 업데이트하는 최적화 알고리즘들(Gradient Descent, SGD, Momentum, Adagrad, RMSProp, Adam)의 원리와 특징을 설명하고 비교한다"}}}]}]
6:["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/911e6a603adbdfb3.css","precedence":"next"}]
7:["$","$L9",null,{"children":["$","$a",null,{"name":"Next.MetadataOutlet","children":"$@b"}]}]
b:null
